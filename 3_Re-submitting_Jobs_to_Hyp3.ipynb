{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5f31db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import requests\n",
    "import hyp3_sdk\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from copy import deepcopy\n",
    "import requests\n",
    "import itertools\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb356a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77889399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isce_jobs4resubmit(jobs_dicts:list):\n",
    "    job_definition = {'name': None,\n",
    "                      'job_parameters': None,\n",
    "                      'job_type': 'INSAR_ISCE'\n",
    "                     }\n",
    "\n",
    "    prepared_jobs = []\n",
    "    for job in jobs_dicts:\n",
    "        prepared_job = deepcopy(job_definition)\n",
    "        prepared_job['name'] = job['name']\n",
    "        prepared_job['job_parameters'] = job['job_parameters']\n",
    "        prepared_job['job_type'] = job['job_type']\n",
    "        prepared_jobs.append(prepared_job)\n",
    "    return prepared_jobs\n",
    "\n",
    "def raider_jobs4resubmit(jobs_dicts:list):\n",
    "    job_definition = {\n",
    "        'name': 're-run RAiDER step',  # optional: provide a name\n",
    "        \"job_type\": \"ARIA_RAIDER\",\n",
    "        \"job_parameters\": {\n",
    "            # \"job_id\": \"27836b79-e5b2-4d8f-932f-659724ea02c3\",\n",
    "            # \"weather_model\": \"ERA5\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "    prepared_jobs = []\n",
    "    for job in jobs_dicts:\n",
    "        prepared_job = deepcopy(job_definition)  \n",
    "        if job['job_type'] == 'INSAR_ISCE':\n",
    "            prepared_job['name'] = job['name'] + '_rerun_raider'\n",
    "            prepared_job['job_parameters']['job_id'] = job['job_id']\n",
    "        else:\n",
    "            prepared_job['name'] = job['name'] \n",
    "            prepared_job['job_parameters']['job_id'] = job['job_parameters']['job_id']\n",
    "        prepared_job['job_parameters']['weather_model'] = job['job_parameters']['weather_model'] \n",
    "        prepared_jobs.append(prepared_job)\n",
    "    return prepared_jobs\n",
    "\n",
    "def split_failed_jobs(failed_jobs):\n",
    "    flag_insar_job = lambda x, y: (x.job_type == 'INSAR_ISCE') & (len(x.processing_times)==y)\n",
    "    # isce\n",
    "    isce_jobs = [j.to_dict() for j in failed_jobs if flag_insar_job(j, 1)]\n",
    "    # raider\n",
    "    raider_jobs_new = [j.to_dict() for j in failed_jobs if flag_insar_job(j, 2)] \n",
    "    raider_jobs = [j.to_dict() for j in failed_jobs if j.job_type == 'ARIA_RAIDER']\n",
    "    raider_jobs = raider_jobs_new + raider_jobs\n",
    "    return isce_jobs, raider_jobs\n",
    "\n",
    "def get_request_date(job_dict):\n",
    "    job_date = datetime.datetime.strptime(job_dict['request_time'], '%Y-%m-%dT%H:%M:%S%z')\n",
    "    return datetime.datetime.strftime(job_date,'%Y-%m-%d') \n",
    "\n",
    "# TODO: add function to find duplicate jobs to filter out same jobs\n",
    "# for now track it by date\n",
    "class access_hyp3_resubmit():\n",
    "    def __init__(self, hyp3_jobs, verbose=True):\n",
    "        # Filter to failed jobs\n",
    "        failed_jobs = hyp3_jobs.filter_jobs(succeeded=False, pending=False,\n",
    "                                            running=False, failed=True)\n",
    "        print(f'Failed jobs:\\n {failed_jobs}')\n",
    "\n",
    "        # Failed at topssApp step\n",
    "        self.isce_jobs, self.raider_jobs = split_failed_jobs(failed_jobs)\n",
    "        if verbose:\n",
    "            self.print_info()\n",
    "\n",
    "    def print_info(self):\n",
    "        print(f'Topsapp jobs: {len(self.isce_jobs)}')\n",
    "        print(f' Submission dates: {np.unique(list(map(get_request_date, self.isce_jobs)))}')\n",
    "\n",
    "        # Failed at raider step\n",
    "        print(f'Raider jobs: {len(self.raider_jobs)}')\n",
    "        print(f' Submission dates: {np.unique(list(map(get_request_date, self.raider_jobs)))}')\n",
    "\n",
    "    def prepare_jobs(self, batch_size=200):\n",
    "        self.prepared_isce = isce_jobs4resubmit(self.isce_jobs)\n",
    "        self.prepared_raider = raider_jobs4resubmit(self.raider_jobs)\n",
    "        # isce\n",
    "        if len(self.prepared_isce) > 0:\n",
    "            self.isce_resubmit = [batch for batch in hyp3_sdk.util.chunk(self.prepared_isce, batch_size)]\n",
    "            msg = f'Prepared isce jobs: {len(self.prepared_isce)} in {len(self.isce_resubmit[0])}'\n",
    "            msg += f'x {len(self.isce_resubmit)} batches'\n",
    "            print(msg)\n",
    "        else:\n",
    "            self.isce_resubmit = None \n",
    "        # raider\n",
    "        if len(self.prepared_raider) > 0:\n",
    "            self.raider_resubmit = [batch for batch in hyp3_sdk.util.chunk(self.prepared_raider, batch_size)]\n",
    "            msg = f'Prepared raider jobs: {len(self.prepared_raider)} in {len(self.raider_resubmit[0])}'\n",
    "            msg += f'x {len(self.raider_resubmit)} batches'\n",
    "            print(msg)\n",
    "        else:\n",
    "            self.raider_resubmit = None\n",
    "        \n",
    "    def submit_isce(self):\n",
    "        if self.isce_resubmit is None:\n",
    "            raise ValueError('There are no prepared jobs!') \n",
    "        print(f'Submitting: {len(self.prepared_isce)} jobs')\n",
    "        for batch in tqdm(self.isce_resubmit):\n",
    "            hyp3_isce.submit_prepared_jobs(batch)\n",
    "\n",
    "    def submit_raider(self):\n",
    "        if self.raider_resubmit is None:\n",
    "            raise ValueError('There are no prepared jobs!') \n",
    "        print(f'Submitting: {len(self.prepared_raider)} jobs')\n",
    "        for batch in tqdm(self.raider_resubmit):\n",
    "            hyp3_isce.submit_prepared_jobs(batch) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2527c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering functions\n",
    "def _exist(failed_job, filt_job_list):\n",
    "    return failed_job['job_parameters'] in filt_job_list\n",
    "\n",
    "# Filter based on status code\n",
    "def filter_failed_jobs(failed_jobs:list, jobs: hyp3_sdk.jobs.Batch, n_jobs:int=10, verbose:bool = False):\n",
    "    # Get succeded, running and pending\n",
    "    filt_job_list = [j.job_parameters for j in jobs.filter_jobs().jobs]\n",
    "    if verbose: print(f'Succedded,pending,running jobs: {len(filt_job_list)}')\n",
    "    if verbose: print(f'Failed jobs: {len(failed_jobs)}')\n",
    "    \n",
    "    # Check if exists in parallel \n",
    "    with Pool(n_jobs) as pool:\n",
    "        flags = pool.starmap(_exist, zip(failed_jobs, itertools.repeat(filt_job_list)))\n",
    "    if verbose: print(f'Failed jobs that exist in above: {np.count_nonzero(np.atleast_1d(flags))}')\n",
    "\n",
    "    # Filter \n",
    "    filt_failed = list(itertools.compress(failed_jobs, ~np.bool_(flags)))\n",
    "    if verbose: print(f'Number of filtered failed jobs: {len(filt_failed)}')\n",
    "\n",
    "    return filt_failed\n",
    "\n",
    "# Filter duplicates\n",
    "# NOTE: wrote this fast, probably could be done better\n",
    "request_date2dt = lambda x: datetime.datetime.strptime(x['request_time'],\n",
    "                                                         '%Y-%m-%dT%H:%M:%S%z')\n",
    "def find_latest_datetime_index(datetimes):\n",
    "    if not datetimes:\n",
    "        return None\n",
    "    \n",
    "    latest_index = 0\n",
    "    latest_datetime = datetimes[0]\n",
    "    \n",
    "    for i, dt in enumerate(datetimes[1:], start=1):\n",
    "        if dt > latest_datetime:\n",
    "            latest_datetime = dt\n",
    "            latest_index = i\n",
    "    \n",
    "    return latest_index\n",
    "\n",
    "def get_job_last_requested(duplicate_jobs: list): \n",
    "    dates = [request_date2dt(d) for d in duplicate_jobs]\n",
    "    ix = find_latest_datetime_index(dates)\n",
    "    return duplicate_jobs[ix]\n",
    "\n",
    " \n",
    "def remove_duplicates(job_list: list):\n",
    "    # strip to only job_parameters\n",
    "    jobs2count = [j['job_parameters'] for j in job_list]\n",
    "\n",
    "    # Get count for jobs\n",
    "    count = {} \n",
    "    for ix, j1 in enumerate(jobs2count):\n",
    "        index = []\n",
    "        for ik, j2 in enumerate(jobs2count):\n",
    "            if  j1 == j2:\n",
    "                index.append(ik)\n",
    "        count[ix] = index\n",
    "\n",
    "    #Filter out same duplicates\n",
    "    for ix in list(count):\n",
    "        if count.get(ix, None) is not None: \n",
    "            for ik in count.get(ix, None):\n",
    "                if ik > ix:\n",
    "                    count.pop(ik, None)\n",
    "\n",
    "    # Loop through duplicates and get job last request\n",
    "    filt_jobs = []\n",
    "    for ix in count:\n",
    "        duplicates =  [job_list[i] for i in count[ix]]\n",
    "        if len(duplicates) > 1:\n",
    "            filt_jobs.append(get_job_last_requested(duplicates))\n",
    "        else:\n",
    "            filt_jobs.append(duplicates[0])\n",
    "    \n",
    "    return filt_jobs, count\n",
    "\n",
    "def remove_raider_duplicates(job_list: list):\n",
    "    # Strip job_id  \n",
    "    jobs2count = []\n",
    "    for j in job_list:\n",
    "        if j['job_type'] == 'INSAR_ISCE':\n",
    "            jobs2count.append(j['job_id'])\n",
    "        elif j['job_type'] == 'ARIA_RAIDER':\n",
    "            jobs2count.append(j['job_parameters']['job_id'])\n",
    "\n",
    "    # Get unique job_ids\n",
    "    unique, indices, counts = np.unique(jobs2count,\n",
    "                                        return_index=True,\n",
    "                                        return_counts=True)\n",
    "    # Filter unique raider failed jobs\n",
    "    count_1 = np.array(job_list)[indices[counts == 1]]\n",
    "\n",
    "    # Find duplicates raider failed jobs\n",
    "    duplicates = []\n",
    "    for ix in unique[counts>1]: \n",
    "        indices = np.where(ix == np.array(jobs2count))[0]\n",
    "        duplicates.append(indices)\n",
    "\n",
    "    # Loop through duplicates and get the last request job\n",
    "    filt_jobs = []\n",
    "    for ix in duplicates:\n",
    "        dx = [job_list[i] for i in ix]\n",
    "        filt_jobs.append(get_job_last_requested(dx))\n",
    "\n",
    "    filt_jobs = np.concatenate([filt_jobs, count_1])\n",
    "    return filt_jobs.tolist()\n",
    "\n",
    "\n",
    "# Filter based on exit log message\n",
    "# example when job failed at raider step due to requested model that does not exist in that aoi\n",
    "hrr_warning_msg = 'ValueError: HRRR was requested but it is not available in this area'\n",
    "\n",
    "def get_log_exit_msg(log_url, msg):\n",
    "    response = requests.get(log_url, headers = {'Range': 'bytes=-1000'})\n",
    "    return response.text.split('\\n')[-1] == msg\n",
    "\n",
    "# Example notebook to inspect jobs: https://github.com/ACCESS-Cloud-Based-InSAR/Aria-Hyp3-GUNW-Frame-Submission/blob/dev/operational_scripts/__Andrew_Inspection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f45fd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "example on jobs submitted for NISAR calval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18e61723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses .netrc; add `prompt=True` to prompt for credentials;\n",
    "hyp3_isce = hyp3_sdk.HyP3('https://hyp3-a19-jpl.asf.alaska.edu', prompt=True)\n",
    "# hyp3_isce = hyp3_sdk.HyP3('https://hyp3-tibet-jpl.asf.alaska.edu', prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90339a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_flags = dict(succeeded=False, pending=False,\n",
    "                        running=False, failed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c698254",
   "metadata": {},
   "source": [
    "first jobs were submitted on May 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43b52ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63861 HyP3 Jobs: 14695 succeeded, 49166 failed, 0 running, 0 pending.\n",
      "##########\n",
      "Failed jobs:\n",
      " 49166 HyP3 Jobs: 0 succeeded, 49166 failed, 0 running, 0 pending.\n",
      "Topsapp jobs: 45687\n",
      " Submission dates: ['2024-05-08']\n",
      "Raider jobs: 3392\n",
      " Submission dates: ['2024-05-08']\n",
      "##########\n",
      "Raider\n",
      "0 HyP3 Jobs: 0 succeeded, 0 failed, 0 running, 0 pending.\n"
     ]
    }
   ],
   "source": [
    "# May-08 to May 09 \n",
    "\n",
    "# request separate to differeniate between topsApp and raider jobs\n",
    "jobs = hyp3_isce.find_jobs(start=datetime.datetime(2024, 5, 8), \n",
    "                           end=datetime.datetime(2024, 5, 9),\n",
    "                           user_id='access_cloud_based_insar',\n",
    "                           job_type='INSAR_ISCE')\n",
    "print(jobs)\n",
    "print('#' * 10)\n",
    "jobs = access_hyp3_resubmit(jobs.filter_jobs(**filtering_flags))\n",
    "\n",
    "raider_jobs = hyp3_isce.find_jobs(user_id='access_cloud_based_insar',\n",
    "                           start=datetime.datetime(2024, 5, 8), \n",
    "                           end=datetime.datetime(2024, 5, 9),\n",
    "                           job_type='ARIA_RAIDER')\n",
    "print('#' * 10)\n",
    "print('Raider')\n",
    "print(raider_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcfa69",
   "metadata": {},
   "source": [
    "in total, 63861 jobs were submitted, 49166 failed of which 45687 failed at topsApp step and 3392 at raider step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bc5b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45687 HyP3 Jobs: 36170 succeeded, 9517 failed, 0 running, 0 pending.\n",
      "##########\n",
      "Failed jobs:\n",
      " 9517 HyP3 Jobs: 0 succeeded, 9517 failed, 0 running, 0 pending.\n",
      "Topsapp jobs: 8390\n",
      " Submission dates: ['2024-05-11']\n",
      "Raider jobs: 1011\n",
      " Submission dates: ['2024-05-11']\n",
      "##########\n",
      "Raider\n",
      "3392 HyP3 Jobs: 0 succeeded, 3392 failed, 0 running, 0 pending.\n"
     ]
    }
   ],
   "source": [
    "# second run\n",
    "jobs = hyp3_isce.find_jobs(start=datetime.datetime(2024, 5, 10), \n",
    "                           end=datetime.datetime(2024, 5, 15),\n",
    "                           user_id='access_cloud_based_insar',\n",
    "                           job_type='INSAR_ISCE')\n",
    "print(jobs)\n",
    "print('#' * 10)\n",
    "jobs = access_hyp3_resubmit(jobs.filter_jobs(**filtering_flags))\n",
    "\n",
    "raider_jobs = hyp3_isce.find_jobs(user_id='access_cloud_based_insar',\n",
    "                           start=datetime.datetime(2024, 5, 10), \n",
    "                           end=datetime.datetime(2024, 5, 15),\n",
    "                           job_type='ARIA_RAIDER')\n",
    "print('#' * 10)\n",
    "print('Raider')\n",
    "print(raider_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc4c7651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54077, 7795)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45687+8390, 3392+1011+3392"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a3ecd",
   "metadata": {},
   "source": [
    "after second run we have 50865 of 63861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "396513f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique succeded aria_raider jobs\n",
    "np.unique([j.job_parameters['job_id'] for j in raider_jobs.filter_jobs(**{**filtering_flags, **{'succeeded':True, 'failed':False}}).jobs]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b283a26f",
   "metadata": {},
   "source": [
    "## Prepare for re-submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rjobs = hyp3_isce.find_jobs(start=datetime.datetime(2024, 5, 10), \n",
    "                           end=datetime.datetime(2024, 5, 15),\n",
    "                           user_id='access_cloud_based_insar')\n",
    "print(rjobs)\n",
    "print('#' * 10)\n",
    "rjobs = access_hyp3_resubmit(rjobs.filter_jobs(**filtering_flags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53db05",
   "metadata": {},
   "source": [
    "### Get raider jobs  with wrong weather model submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5defef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I submitted HRRR over Hawaii and Alaska where is not HRRR\n",
    "logs = [j['logs'][0] for j in rjobs.raider_jobs if len(j['logs']) > 0]\n",
    "\n",
    "with Pool(10) as pool:\n",
    "    # Map the function over the logs, passing the warning message to each call\n",
    "    job_flags = pool.starmap(get_log_exit_msg, zip(logs, itertools.repeat(hrr_warning_msg)))\n",
    "\n",
    "print(f'Number of failed raider jobs: {len(list(itertools.compress(logs, job_flags)))} of {len(logs)}')\n",
    "print(f' with msg: {hrr_warning_msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find jobs that failed with defined msg\n",
    "nohrrr_jobs = list(itertools.compress(rjobs.raider_jobs, job_flags))\n",
    "\n",
    "# Find tracks, tracks 124, 87 (Hawaii), 44 (Northern Alaska Slopes)\n",
    "np.unique([j['name'] for j in nohrrr_jobs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aae078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Get job_ids to give ASF for manual import to DAAC\n",
    "hrrr_jobs = []\n",
    "\n",
    "for j in nohrrr_jobs:\n",
    "    if j['job_type'] == 'INSAR_ISCE':\n",
    "        hrrr_jobs.append(j['job_id'])\n",
    "    elif j['job_type'] == 'ARIA_RAIDER':\n",
    "        hrrr_jobs.append(j['job_parameters']['job_id'])\n",
    "\n",
    "hrrr_jobs = pd.DataFrame(hrrr_jobs, columns=['job_id'])\n",
    "hrrr_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrr_jobs.to_csv('access_nohrrr_job_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid submitting this jobs again, filter them out\n",
    "job_flags2 = [~np.bool_(jf) for jf in job_flags]\n",
    "rjobs.raider_jobs = list(itertools.compress(rjobs.raider_jobs, job_flags2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42279a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To submit\n",
    "rjobs.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67298c",
   "metadata": {},
   "source": [
    "### Filtering to avoid tracking submission dates\n",
    "NOTE: need to work on this, does not filter correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "176c813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112940 HyP3 Jobs: 50865 succeeded, 62075 failed, 0 running, 0 pending.\n",
      "##########\n",
      "Failed jobs:\n",
      " 62075 HyP3 Jobs: 0 succeeded, 62075 failed, 0 running, 0 pending.\n",
      "Topsapp jobs: 54077\n",
      " Submission dates: ['2024-05-08' '2024-05-11']\n",
      "Raider jobs: 7795\n",
      " Submission dates: ['2024-05-08' '2024-05-11']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n####### Filter failed jobs that succeeded, are pending or running ####### \\nprint('#' * 10)\\nverbose = False\\n\\nif verbose: print('Failed jobs at topsApp step')\\naccess_jobs.isce_jobs = filter_failed_jobs(access_jobs.isce_jobs, all_jobs,\\n                                           n_jobs=20, verbose=verbose)\\nif verbose: print('\\nFailed jobs at raider step')\\naccess_jobs.raider_jobs = filter_failed_jobs(access_jobs.raider_jobs, all_jobs,\\n                                             n_jobs=20, verbose=verbose)\\n\\nprint('\\nAfter filtering already succedded, pending, running')\\nprint('#' * 10)\\naccess_jobs.print_info()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last run\n",
    "all_jobs = hyp3_isce.find_jobs(start=datetime.datetime(2024, 5, 8), \n",
    "                           end=datetime.datetime(2024, 5, 15),\n",
    "                           user_id='access_cloud_based_insar')\n",
    "print(all_jobs)\n",
    "print('#' * 10)\n",
    "access_jobs = access_hyp3_resubmit(all_jobs.filter_jobs(**filtering_flags))\n",
    "\n",
    "####### Filter failed jobs that succeeded, are pending or running ####### \n",
    "print('#' * 10)\n",
    "verbose = False\n",
    "\n",
    "if verbose: print('Failed jobs at topsApp step')\n",
    "access_jobs.isce_jobs = filter_failed_jobs(access_jobs.isce_jobs, all_jobs,\n",
    "                                           n_jobs=20, verbose=verbose)\n",
    "if verbose: print('\\nFailed jobs at raider step')\n",
    "access_jobs.raider_jobs = filter_failed_jobs(access_jobs.raider_jobs, all_jobs,\n",
    "                                             n_jobs=20, verbose=verbose)\n",
    "\n",
    "print('\\nAfter filtering already succedded, pending, running')\n",
    "print('#' * 10)\n",
    "access_jobs.print_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee2034eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45687,), 54077)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way of doing deduplication, hashing frame_id with ref_sec dates\n",
    "def _get_gunw_pairing(job_dict):\n",
    "    date1 = job_dict['granules'][0][17:25]\n",
    "    date2 = job_dict['secondary_granules'][0][17:25]\n",
    "    return date1 + '_' + date2\n",
    "\n",
    "def dict2hash(job_dict:dict):\n",
    "    return hash((job_dict['frame_id'], _get_gunw_pairing(job_dict)))\n",
    "\n",
    "# Try to filter with this\n",
    "jobs2count = [j['job_parameters'] for j in access_jobs.isce_jobs]\n",
    "np.unique(list(map(dict2hash, jobs2count))).shape, len(access_jobs.isce_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b01e9af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9517, 17907)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_duplicates(access_jobs.isce_jobs)[0]), len(access_jobs.isce_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e733028e",
   "metadata": {},
   "source": [
    "work in the same way, but somehow I am missing to remove 1127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "41e2ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_jobs = remove_duplicates(access_jobs.isce_jobs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8dfe4775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-05-08' '2024-05-11']\n",
      "1127\n",
      "8390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check based on request dates\n",
    "request_dates = list(map(get_request_date, filt_jobs))\n",
    "unique_date = np.unique(request_dates)\n",
    "print(unique_date)\n",
    "ix1 = np.where(np.array('2024-05-08') == np.array(request_dates))\n",
    "ix2 = np.where(np.array('2024-05-11') == np.array(request_dates))\n",
    "print(ix1[0].shape[0])\n",
    "print(ix2[0].shape[0])\n",
    "# So can see that 1127 in 20240508 are not removed\n",
    "\n",
    "\n",
    "d1 = np.array(filt_jobs)[ix1]\n",
    "d2= np.array(filt_jobs)[ix2]\n",
    "\n",
    "d11 = [d['job_parameters'] for d in d1]\n",
    "d22 = [d['job_parameters'] for d in d2]\n",
    "\n",
    "# cannot find where it leaks \n",
    "np.count_nonzero([i in d22 for i in d11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cbea519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After filtering failed jobs duplicates\n",
      "##########\n",
      "Topsapp jobs: 45687\n",
      " Submission dates: ['2024-05-08' '2024-05-11']\n",
      "Raider jobs: 4403\n",
      " Submission dates: ['2024-05-11']\n"
     ]
    }
   ],
   "source": [
    "####### Filter duplicated failed jobs ####### \n",
    "access_jobs.isce_jobs = remove_duplicates(access_jobs.isce_jobs)[0]\n",
    "access_jobs.raider_jobs = remove_raider_duplicates(access_jobs.raider_jobs)\n",
    "\n",
    "print('\\nAfter filtering failed jobs duplicates')\n",
    "print('#' * 10)\n",
    "access_jobs.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d7309",
   "metadata": {},
   "source": [
    "## Re-submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare jobs for re-submission\n",
    "rjobs.prepare_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa4441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this goes faster\n",
    "rjobs.submit_raider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rjobs.submit_isce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "succeded_flag = {**filtering_flags, **{'succeeded':True, 'failed':False}}\n",
    "unique_succeded = np.unique([j.job_parameters['job_id'] for j in raider_jobs.filter_jobs(**succeded_flag).jobs])\n",
    "unique_failed = np.unique([j.job_parameters['job_id'] for j in raider_jobs.filter_jobs(**filtering_flags).jobs])\n",
    "# NOTE compare ARIA_RAIDER with INSAR_ISCE job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce06e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8390 HyP3 Jobs: 0 succeeded, 1823 failed, 6250 running, 317 pending.\n",
      "##########\n",
      "Failed jobs:\n",
      " 1823 HyP3 Jobs: 0 succeeded, 1823 failed, 0 running, 0 pending.\n",
      "Topsapp jobs: 1823\n",
      " Submission dates: ['2024-05-17']\n",
      "Raider jobs: 0\n",
      " Submission dates: []\n",
      "##########\n",
      "Raider\n",
      "119 HyP3 Jobs: 80 succeeded, 39 failed, 0 running, 0 pending.\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "jobs = hyp3_isce.find_jobs(start=datetime.datetime(2024, 5, 16), \n",
    "                           end=datetime.datetime(2024, 5, 18),\n",
    "                           user_id='access_cloud_based_insar',\n",
    "                           job_type='INSAR_ISCE')\n",
    "print(jobs)\n",
    "print('#' * 10)\n",
    "jobs = access_hyp3_resubmit(jobs.filter_jobs(**filtering_flags))\n",
    "\n",
    "raider_jobs = hyp3_isce.find_jobs(user_id='access_cloud_based_insar',\n",
    "                           start=datetime.datetime(2024, 5, 16), \n",
    "                           end=datetime.datetime(2024, 5, 18),\n",
    "                           job_type='ARIA_RAIDER')\n",
    "print('#' * 10)\n",
    "print('Raider')\n",
    "print(raider_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc20d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "raider_jobs.filter_jobs(**filtering_flags).jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea33673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check failed jobs\n",
    "print(f'# failed jobs {len(rjobs.raider_jobs)}')\n",
    "all_logs = [j['logs'] for j in rjobs.raider_jobs]\n",
    "all_logs[0], all_logs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4f81c",
   "metadata": {},
   "source": [
    "## asf search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asf_search as asf\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc26e3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires asf_search > 7.0.8\n",
    "opts = asf.ASFSearchOptions(\n",
    "    #shortName='ARIA_S1_GUNW', # version 3\n",
    "    #relativeOrbit=[13],\n",
    "    #flightDirection='DESCENDING',\n",
    "    processingLevel='GUNW_STD',\n",
    "    intersectsWith='POLYGON((-127.992 32.2865,-115.5115 32.2865,-115.5115 49.1812,-127.992 49.1812,-127.992 32.2865))',\n",
    "    collectionAlias=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fcd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requires asf_search > 7.0.8\n",
    "opts = asf.ASFSearchOptions(\n",
    "    shortName='ARIA_S1_GUNW', # version 3\n",
    "    #relativeOrbit=[13],\n",
    "    #flightDirection='DESCENDING',\n",
    "    #processingLevel='GUNW_STD',\n",
    "    intersectsWith='POLYGON((-118.5619 33.4925,-117.6485 33.4925,-117.6485 34.2148,-118.5619 34.2148,-118.5619 33.4925))',\n",
    "    collectionAlias=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = asf.search(opts=opts)\n",
    "gdf = gpd.GeoDataFrame.from_features(scenes.geojson(), crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee729a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[gdf.fileID.apply(lambda x: x.split('-')[6]) == '20190123_20190117'].url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28594d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = gdf.fileID.apply(lambda x: int(x.split('-')[-1].split('_')[0].split('v')[-1]))\n",
    "version.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aria_hyp3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
